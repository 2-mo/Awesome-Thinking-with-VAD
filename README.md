# Awesome Thinking with VAD üß†üé•

[![Awesome](https://awesome.re/badge.svg)](https://awesome.re)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

> A curated collection of research papers and resources exploring **thoughtful reasoning approaches** in Video Anomaly Detection (VAD), with special focus on Large Language Models (LLMs) and Vision-Language Models (VLMs).

## üìñ Table of Contents

- [Awesome Thinking with VAD üß†üé•](#awesome-thinking-with-vad-)
  - [üìñ Table of Contents](#-table-of-contents)
  - [ü§ù Stay Connected](#-stay-connected)
  - [üåü Overview](#-overview)
  - [üß≠ How to Use This Repository](#-how-to-use-this-repository)
    - [üìÇ Repository Map](#-repository-map)
    - [üìù Reading Workflow](#-reading-workflow)
  - [üìö Conference Snapshots](#-conference-snapshots)
  - [üì∞ Journal Snapshots](#-journal-snapshots)
  - [üìä Benchmarks and Datasets](#-benchmarks-and-datasets)
    - [ü§ñ LLM/VLM-Ready Datasets (Multimodal \& Explainable)](#-llmvlm-ready-datasets-multimodal--explainable)
      - [üìù Video-Language Annotation](#-video-language-annotation)
      - [üîç Anomaly Retrieval (Cross-modal)](#-anomaly-retrieval-cross-modal)
      - [üåê Open-World Understanding](#-open-world-understanding)
      - [üé¨ Large-Scale Multimodal](#-large-scale-multimodal)
    - [üîß Traditional VAD Benchmarks](#-traditional-vad-benchmarks)
      - [1Ô∏è‚É£ Weakly Supervised](#1Ô∏è‚É£-weakly-supervised)
      - [2Ô∏è‚É£ Semi-supervised](#2Ô∏è‚É£-semi-supervised)
      - [3Ô∏è‚É£ Fully Supervised](#3Ô∏è‚É£-fully-supervised)
    - [üöó Domain-Specific Datasets](#-domain-specific-datasets)
      - [Driving \& Transportation](#driving--transportation)
      - [Multi-Scenario](#multi-scenario)
  - [üîó Related Resources](#-related-resources)
    - [Related Awesome Lists](#related-awesome-lists)
  - [ü§ù Contributing](#-contributing)
  - [üìú License and Credits](#-license-and-credits)

---

## ü§ù Stay Connected

> Êâ´Á†ÅÂä†ÂÖ•Â∞èÁ∫¢‰π¶„ÄåËßÜÈ¢ëÂºÇÂ∏∏Ê£ÄÊµã„Äç‰∫§ÊµÅÂúàÔºåÂàÜ‰∫´ËÆ∫Êñá„ÄÅÂ∑•‰Ωú‰∏éÂøÉÂæó‰Ωì‰ºö„ÄÇ

<div align="center">
  <img src="assets/qrcode/redbook[25.11.25].JPG" alt="Thinking with VAD Xiaohongshu QR code" width="220">
  <p><em>[RedBook] until 2025-Nov-25</em></p>
</div>

---

## üåü Overview

Modern video anomaly detection is moving beyond frame-level alarms toward systems that **interpret, justify, and communicate** why something looks suspicious. This list mirrors that shift by collecting papers, datasets, and tooling that emphasize reasoning-heavy VAD pipelines‚Äîespecially those powered by large language models (LLMs) and vision-language models (VLMs).

**What you can expect:**
- Curated reading paths that show how ‚Äúslow thinking‚Äù modules (reasoning, explanation, planning) complement classic perception backbones
- Venue and journal snapshots that surface where thinking-centric VAD work is appearing and how the discussion is evolving
- Dataset groupings that make it straightforward to pick between LLM-ready benchmarks and traditional baselines when scoping a project or reproduction study

**Who this is for:** researchers, students, and practitioners who want a single hub for tracking the convergence of anomaly detection, multimodal understanding, and foundation models.

## üß≠ How to Use This Repository

Treat this repo as a launchpad. Start with the overview for a quick mental model, then dive into venue or journal notes, or jump straight to the dataset taxonomy depending on whether you need new data, baselines, or multimodal annotations.

### üìÇ Repository Map
- `venues/` ‚Äî year-by-year highlights from major conferences with takeaways and trend notes
- `journals/` ‚Äî rolling coverage of influential journal publications grouped by outlet for faster literature sweeps
- `assets/` ‚Äî figures and badges used across the documentation if you want to reuse the styling in decks or reports

### üìù Reading Workflow
1. Skim the **LLM/VLM-ready datasets** when prototyping reasoning-enabled pipelines; drop to the traditional benchmarks for baselines or model comparisons.
2. Jump into the linked venue or journal page when you need richer context such as paper clusters, methodological trends, or open questions.
3. Open an issue or PR whenever you spot a missing reference‚Äîreasoning-centric resources ship rapidly, so community updates keep the list fresh.

---

## üìö Conference Snapshots

The `venues/` directory hosts per-conference notes for 2023-2025. Quick links:

- [CVPR](venues/cvpr.md) ‚Äî Computer Vision and Pattern Recognition
- [ICCV](venues/iccv.md) ‚Äî International Conference on Computer Vision
- [ECCV](venues/eccv.md) ‚Äî European Conference on Computer Vision
- [NeurIPS](venues/neurips.md) ‚Äî Neural Information Processing Systems
- [ICML](venues/icml.md) ‚Äî International Conference on Machine Learning
- [ICLR](venues/iclr.md) ‚Äî International Conference on Learning Representations
- [AAAI](venues/aaai.md) ‚Äî Association for the Advancement of Artificial Intelligence
- [IJCAI](venues/ijcai.md) ‚Äî International Joint Conference on Artificial Intelligence
- [ACM MM](venues/acmmm.md) ‚Äî ACM Multimedia

---

## üì∞ Journal Snapshots

The `journals/` directory hosts per-journal notes for top-tier academic journals. Quick links:

- [TPAMI](journals/tpami.md) ‚Äî IEEE Transactions on Pattern Analysis and Machine Intelligence
- [TIP](journals/tip.md) ‚Äî IEEE Transactions on Image Processing
- [TNNLS](journals/tnnls.md) ‚Äî IEEE Transactions on Neural Networks and Learning Systems
- [TCYB](journals/tcyb.md) ‚Äî IEEE Transactions on Cybernetics
- [TIFS](journals/tifs.md) ‚Äî IEEE Transactions on Information Forensics and Security
- [IJCV](journals/ijcv.md) ‚Äî International Journal of Computer Vision (Springer)

---

## üìä Benchmarks and Datasets

> üí° **Trend**: Datasets are evolving from pure detection (traditional) to **understanding + explanation** (LLM-ready), aligning with the shift from "fast perception" to "slow thinking" in anomaly detection.


### ü§ñ LLM/VLM-Ready Datasets (Multimodal & Explainable)

Datasets designed for or compatible with large language models and vision-language models, emphasizing reasoning, explanation, and multimodal understanding.

#### üìù Video-Language Annotation
- **[UCA (UCF-Crime Annotation)](https://xuange923.github.io/Surveillance-Video-Understanding)** (CVPR 2024)
  - 23,542 fine-grained sentences, 111 hours
  - Temporal event descriptions for surveillance video understanding
  
- **[VAD-Instruct50k](https://holmesvad.github.io/)** (Holmes-VAD, arXiv 2024)
  - 51,567 multimodal instructions for explainable VAD
  - Rich textual explanations for anomaly reasoning
  
- **[UCCD](https://github.com/lingruzhou/UCCD)** (TMM 2024)
  - Human-centric behavior descriptions
  - Instance-level annotations with temporal info

#### üîç Anomaly Retrieval (Cross-modal)
- **[UCFCrime-AR](https://github.com/Roc-Ng/VAR)** (TIP 2024)
  - Video-text retrieval benchmark
  - 1,900 videos with Chinese & English descriptions
  
- **[XDViolence-AR](https://github.com/Roc-Ng/VAR)** (TIP 2024)
  - Audio-visual anomaly retrieval
  - 4,754 videos, cross-modal (video ‚Üî audio)

#### üåê Open-World Understanding
- **[UBnormal](https://github.com/lilygeorgescu/UBnormal)** (CVPR 2022)
  - Open-set benchmark: 22 training anomaly types, disjoint from test
  - 543 videos with pixel-level annotations
  - Simulates real-world unseen anomaly scenarios

#### üé¨ Large-Scale Multimodal
- **[XD-Violence](https://roc-ng.github.io/XD-Violence/)** (ECCV 2020)
  - 4,754 videos, 217 hours, 6 violence types
  - **Audio-visual** modality (speech, sound effects)
  - Suitable for multimodal LLM/VLM approaches

---

### üîß Traditional VAD Benchmarks

Classic datasets used for traditional deep learning and rule-based methods, organized by supervision type.

#### 1Ô∏è‚É£ Weakly Supervised
*Video-level labels only, no frame-level annotations.*

- **[UCF-Crime](https://www.crcv.ucf.edu/projects/real-world/)** (CVPR 2018) ‚Äî 1,900 videos, 128 hours, 13 anomaly types
- **[ShanghaiTech Weakly](https://github.com/jx-zhong-for-academic-purpose/GCN-Anomaly-Detection)** (CVPR 2019) ‚Äî Reorganized for weakly supervised setting
- **[TAD](https://github.com/ktr-hubrt/WSAL)** (TIP 2021) ‚Äî 500 traffic videos, 7 road anomaly types

#### 2Ô∏è‚É£ Semi-supervised
*Train on normal videos only, detect anomalies at test time.*

**Classic Benchmarks:**
- **[UCSD Ped1 & Ped2](http://www.svcl.ucsd.edu/projects/anomaly/dataset.htm)** (CVPR 2010) ‚Äî Pedestrian anomalies
- **[CUHK Avenue](https://www.cse.cuhk.edu.hk/leojia/projects/detectabnormal/dataset.html)** (ICCV 2013) ‚Äî 37 sequences, running/throwing
- **[ShanghaiTech](https://svip-lab.github.io/dataset/campus_dataset.html)** (ICCV 2017) ‚Äî 437 videos, 13 scenes, campus surveillance
- **[UMN](https://www.crcv.ucf.edu/research/projects/abnormal-crowd-behavior-detection-using-social-force-model/)** (CVPR 2009) ‚Äî Escape events

**Recent Large-scale:**
- **[NWPU Campus](https://campusvad.github.io/)** (CVPR 2023) ‚Äî **Largest**: 547 videos, 16 hours, 43 scenes, 28 classes
  - Scene-dependent anomalies
  - Anomaly anticipation task
- **[Street Scene](https://www.merl.com/research/highlights/video-anomaly-detection)** (WACV 2020) ‚Äî 205 anomalies, 17 types
- **[IITB-Corridor](https://rodrigues-royston.github.io/Multi-timescale_Trajectory_Prediction/)** (WACV 2020) ‚Äî Group-level anomalies

**Others:**
- **Subway Entrance & Exit** (TPAMI 2008) ‚Äî Early surveillance benchmarks

#### 3Ô∏è‚É£ Fully Supervised
*Both normal and abnormal videos for training.*

- **[Hockey Fight & Movies Fight](https://academictorrents.com/details/70e0794e2292fc051a13f05ea6f5b6c16f3d3635)** (CAIP 2011) ‚Äî 1,000 + 200 clips
- **[Violent-Flows](https://www.openu.ac.il/home/hassner/data/violentflows/)** (CVPR Workshops 2012) ‚Äî Violent crowd behavior
- **[RWF-2000](https://github.com/mchengny/RWF2000-Video-Database-for-Violence-Detection)** (ICPR 2020) ‚Äî 2,000 videos
- **[CCTV-Fights](https://rose1.ntu.edu.sg/dataset/cctvFights/)** (ICASSP 2019) ‚Äî 1,000 real-world fights
- **[VFD-2000](https://github.com/Hepta-Col/VideoFightDetection)** (ICTAI 2022) ‚Äî Multi-scenario, various lengths
- **[VSD](https://www.interdigital.com/data_sets/violent-scenes-dataset)** (MTA 2015) ‚Äî 18 Hollywood movies

---

### üöó Domain-Specific Datasets

#### Driving & Transportation
- **[Honda HDD](https://usa.honda-ri.com/hdd#Videos)** ‚Äî Driving anomaly detection
- **[ROADWork](https://www.cs.cmu.edu/~roadwork/)** (ICCV 2025) ‚Äî Work zone recognition
- **[Driver Anomaly Detection](https://github.com/okankop/Driver-Anomaly-Detection)** ‚Äî Driver behavior analysis
- **[TAD](https://github.com/ktr-hubrt/WSAL)** (TIP 2021) ‚Äî Traffic anomaly detection

#### Multi-Scenario
- **[MSAD](https://msad-dataset.github.io/)** (NeurIPS 2024) [![arXiv](https://img.shields.io/badge/arXiv-2402.04857-b31b1b)](https://arxiv.org/pdf/2402.04857) ‚Äî Multi-scenario, large-scale







---

## üîó Related Resources

### Related Awesome Lists

- [![Awesome-Anomaly-Detection-Foundation-Models](https://img.shields.io/badge/Awesome-Anomaly_Detection_Foundation_Models-black?logo=github)](https://github.com/mala-lab/Awesome-Anomaly-Detection-Foundation-Models)
- [![Awesome-Video-Anomaly-Detection](https://img.shields.io/badge/Awesome-Video_Anomaly_Detection-black?logo=github)](https://github.com/fjchange/awesome-video-anomaly-detection)
- [![Deep-Learning-Based-Anomaly-Detection](https://img.shields.io/badge/Awesome-Deep_Learning_Anomaly_Detection-black?logo=github)](https://github.com/bitzhangcy/Deep-Learning-Based-Anomaly-Detection)
- [![Awesome-Temporal-Video-Grounding](https://img.shields.io/badge/Awesome-Temporal_Video_Grounding-black?logo=github)](https://github.com/Tangkfan/Awesome-Temporal-Video-Grounding)


---

## ü§ù Contributing

We welcome contributions! Please feel free to:

- Submit pull requests to add new papers, datasets, or resources
- Open issues for corrections or suggestions
- Share your own work related to thinking-based VAD

**Guidelines:**
- Follow the existing format for paper entries
- Include links to paper, code, and project pages when available
- Add a brief highlight describing the key contribution
- Place papers in the appropriate year and conference section

---

## üìú License and Credits

This collection is maintained as an open resource for the research community. 

- Content is gathered from publicly available sources
- Paper copyrights belong to their respective authors and publishers
- This repository is for academic and educational purposes

**Maintainers**: Feel free to reach out for collaborations or suggestions!

---

**Star ‚≠ê this repo if you find it helpful!**