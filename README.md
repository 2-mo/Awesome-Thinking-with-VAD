# Awesome Thinking with VAD ğŸ§ ğŸ¥

[![Awesome](https://awesome.re/badge.svg)](https://awesome.re)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

> A curated collection of research papers and resources exploring **thoughtful reasoning approaches** in Video Anomaly Detection (VAD), with special focus on Large Language Models (LLMs) and Vision-Language Models (VLMs).

## ğŸ“– Table of Contents

- [Awesome Thinking with VAD ğŸ§ ğŸ¥](#awesome-thinking-with-vad-)
  - [ğŸ“– Table of Contents](#-table-of-contents)
  - [ğŸ¤ Stay Connected](#-stay-connected)
  - [ğŸŒŸ Overview](#-overview)
  - [ğŸ“š Conference Snapshots](#-conference-snapshots)
  - [ğŸ“° Journal Snapshots](#-journal-snapshots)
  - [ï¿½ Benchmarks and Datasets](#-benchmarks-and-datasets)
  - [ğŸ”— Related Resources](#-related-resources)
    - [ç›¸å…³åˆé›†](#ç›¸å…³åˆé›†)
    - [Related Awesome Lists](#related-awesome-lists)
  - [ğŸ¤ Contributing](#-contributing)
  - [ğŸ“œ License and Credits](#-license-and-credits)

---

## ğŸ¤ Stay Connected

> æ‰«ç åŠ å…¥å°çº¢ä¹¦ã€Œè§†é¢‘å¼‚å¸¸æ£€æµ‹ã€äº¤æµåœˆï¼Œåˆ†äº«è®ºæ–‡ã€å·¥ä½œä¸å¿ƒå¾—ä½“ä¼šã€‚

<div align="center">
  <img src="assets/qrcode/redbook[25.11.25].JPG" alt="Thinking with VAD Xiaohongshu QR code" width="220">
  <p><em>[RedBook] until 2025-Nov-25</em></p>
  <p>ğŸ“± å¾®ä¿¡å·ï¼š<strong>tiumo-</strong></p>
</div>

---

> ğŸš§ **This repository is under active construction.** We're continuously adding new papers, refining categorizations, and expanding dataset coverage. Stay tuned for updates!

## ğŸŒŸ Overview

Video anomaly detection is evolving from simple frame-level alerts to systems that **reason, explain, and communicate** what makes something suspicious. This repository tracks that shift, focusing on methods that leverage **Large Language Models (LLMs)** and **Vision-Language Models (VLMs)** for deeper anomaly understanding.

**What's inside:**
- ğŸ“š Conference & journal paper collections organized by venue and year
- ğŸ“Š Datasets categorized by LLM-readiness (explainable annotations vs. traditional labels)
- ğŸ”— Quick navigation to reasoning-centric VAD resources

**For:** researchers and practitioners exploring the intersection of anomaly detection, multimodal reasoning, and foundation models.

---

## ğŸ“š Conference Snapshots

The `venues/` directory hosts per-conference notes for 2023-2025. Quick links:

- [CVPR](venues/cvpr.md) â€” Computer Vision and Pattern Recognition
- [ICCV](venues/iccv.md) â€” International Conference on Computer Vision
- [ECCV](venues/eccv.md) â€” European Conference on Computer Vision
- [NeurIPS](venues/neurips.md) â€” Neural Information Processing Systems
- [ICML](venues/icml.md) â€” International Conference on Machine Learning
- [ICLR](venues/iclr.md) â€” International Conference on Learning Representations
- [AAAI](venues/aaai.md) â€” Association for the Advancement of Artificial Intelligence
- [IJCAI](venues/ijcai.md) â€” International Joint Conference on Artificial Intelligence
- [ACM MM](venues/acmmm.md) â€” ACM Multimedia

---

## ğŸ“° Journal Snapshots

The `journals/` directory hosts per-journal notes for top-tier academic journals. Quick links:

- [TPAMI](journals/tpami.md) â€” IEEE Transactions on Pattern Analysis and Machine Intelligence
- [TIP](journals/tip.md) â€” IEEE Transactions on Image Processing
- [TNNLS](journals/tnnls.md) â€” IEEE Transactions on Neural Networks and Learning Systems
- [TCYB](journals/tcyb.md) â€” IEEE Transactions on Cybernetics
- [TIFS](journals/tifs.md) â€” IEEE Transactions on Information Forensics and Security
- [IJCV](journals/ijcv.md) â€” International Journal of Computer Vision (Springer)




## ï¿½ Benchmarks and Datasets

We maintain a comprehensive catalog of VAD datasets in **[dataset.md](dataset.md)**, organized by:

- ğŸ¤– **LLM/VLM-Ready Datasets** â€” Multimodal & explainable annotations
  - Video-language annotation (UCA, VAD-Instruct50k, UCCD)
  - Cross-modal retrieval (UCFCrime-AR, XDViolence-AR)
  - Open-world understanding (UBnormal)
  - Large-scale multimodal (XD-Violence)

- ğŸ”§ **Traditional VAD Benchmarks** â€” Classic deep learning datasets
  - Weakly supervised (UCF-Crime, ShanghaiTech-W, TAD)
  - Semi-supervised (UCSD, Avenue, ShanghaiTech, NWPU Campus)
  - Fully supervised (Hockey Fight, RWF-2000, CCTV-Fights)

- ğŸš— **Domain-Specific** â€” Driving, traffic, and specialized scenarios
  - Honda HDD, ROADWork, MSAD

ğŸ‘‰ **[View full dataset catalog â†’](dataset.md)**

---

## ğŸ”— Related Resources

### ç›¸å…³åˆé›†

- [ICCV 2025 Tutorial: Foundation Models for Anomaly Detection](https://sites.google.com/view/iccv2025-tutorial-fm-driven-ad/home)

### Related Awesome Lists

- [![Awesome-Anomaly-Detection-Foundation-Models](https://img.shields.io/badge/Awesome-Anomaly_Detection_Foundation_Models-black?logo=github)](https://github.com/mala-lab/Awesome-Anomaly-Detection-Foundation-Models)
- [![Awesome-Video-Anomaly-Detection](https://img.shields.io/badge/Awesome-Video_Anomaly_Detection-black?logo=github)](https://github.com/fjchange/awesome-video-anomaly-detection)
- [![Deep-Learning-Based-Anomaly-Detection](https://img.shields.io/badge/Awesome-Deep_Learning_Anomaly_Detection-black?logo=github)](https://github.com/bitzhangcy/Deep-Learning-Based-Anomaly-Detection)
- [![Awesome-Temporal-Video-Grounding](https://img.shields.io/badge/Awesome-Temporal_Video_Grounding-black?logo=github)](https://github.com/Tangkfan/Awesome-Temporal-Video-Grounding)

---

## ğŸ¤ Contributing

We welcome contributions! Please feel free to:

- Submit pull requests to add new papers, datasets, or resources
- Open issues for corrections or suggestions
- Share your own work related to thinking-based VAD

**Guidelines:**
- Follow the existing format for paper entries
- Include links to paper, code, and project pages when available
- Add a brief highlight describing the key contribution
- Place papers in the appropriate year and conference section

---

## ğŸ“œ License and Credits

This collection is maintained as an open resource for the research community. 

- Content is gathered from publicly available sources
- Paper copyrights belong to their respective authors and publishers
- This repository is for academic and educational purposes

**Maintainers**: Feel free to reach out for collaborations or suggestions!

---

**Star â­ this repo if you find it helpful!**